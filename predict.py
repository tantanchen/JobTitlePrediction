# -*- coding: utf-8 -*-
"""Predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OgukOsp3Lj4mmdZjF6gpceCtNiHHz1Wp
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/MyDrive/Colab Notebooks/Job Analysis')

# Commented out IPython magic to ensure Python compatibility.
!pip install colabcode
!pip install fastapi
# %cd fastText
!pip install .
os.chdir('/content/drive/MyDrive/Colab Notebooks/Job Analysis')

import pandas as pd
import numpy as np
import tensorflow as tf
import io, codecs
import fasttext
import fasttext.util
from pydantic import BaseModel
from fastapi import FastAPI
from tensorflow import keras
from sklearn.metrics.pairwise import cosine_similarity
from keras.preprocessing import sequence
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer

def preprocessData(textList, tokenizer):
  tokenizer.fit_on_texts(textList)
  return sequence.pad_sequences(tokenizer.texts_to_sequences(textList),maxlen=300), tokenizer.word_index

class Text(BaseModel):
    text: str 
    class Config:
      schema_extra = {
          "example": {
              "text": "Here is an example text", 
          }
      }

app = FastAPI()

@app.on_event("startup")
def load_model():
    global model
    global ft
    global Position_Names
    global Position_Names_Vec
    global tokenizer

    MAX_NB_WORDS = 100000

    ft = fasttext.load_model('cc.en.300.bin')
    model = keras.models.load_model('LSTM_model')
    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)

    Position_Names = np.load('/content/drive/MyDrive/Colab Notebooks/Job Analysis/Position_Names.npy', allow_pickle=True)
    Position_Names_Vec = np.load('/content/drive/MyDrive/Colab Notebooks/Job Analysis/Position_Names_Vec.npy', allow_pickle=True)

@app.get('/')
def index():
    return {'message': 'This is the homepage of the API '}


@app.post('/predict')
def predict_job(data: Text):
    
    word_seq, word_index =preprocessData([data.text], tokenizer)
    pred=model.predict(word_seq)
    output = []
    max=0.0
    for i in range(Position_Names_Vec.shape[0]):
      cos= cosine_similarity([pred[0],Position_Names_Vec[i]])[0][1]
      output.append(cos)
      if cos > max:
        max = cos
        print(max)
    dictionary = pd.DataFrame(Position_Names, columns=['Position'])
    dictionary['Cosine'] = output
    dictionary=dictionary.sort_values(by=['Cosine'], ascending=False)
    return {'Position': dictionary.head(10)['Position'].tolist(), 'Cosine': dictionary.head(10)['Cosine'].tolist()}

from colabcode import ColabCode
server = ColabCode(port=10000, code=False)

server.run_app(app=app)

